{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MHadavand/Lessons/blob/master/ML/ANN/GPU_CNN/CNN-MNIST-Dataset_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNo5DJaDi2JC"
   },
   "source": [
    "\n",
    "<h1 align=\"center\"><font size=\"5\">CONVOLUTIONAL NEURAL NETWORK</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:20:38.454291Z",
     "start_time": "2020-09-24T21:20:30.754845Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import rc\n",
    "import seaborn as sn\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "def flatten_images(image):\n",
    "    '''\n",
    "    Flatten an image\n",
    "    '''\n",
    "    original_shape = image.shape # (number_images by y_pixels(rows) by x_pixels(columns))\n",
    "    return image.reshape(original_shape[0], original_shape[1]*original_shape[1])\n",
    "\n",
    "def transform_label(x):\n",
    "    ''' Using the one hot concept to model the 9 discrete possible integer values for classification to get\n",
    "        probability of each class. Using argmax function allows us to get the correct class i.e. 0, 1, 2, ... 9\n",
    "    '''\n",
    "    one_hot = []\n",
    "    for item in x:\n",
    "        one_hot.append([float(int(i==item)) for i in range(0,10)])\n",
    "    return np.array(one_hot)\n",
    "\n",
    "def image_plot(image, digit, cmap='bone_r', figsize=(5,5)):\n",
    "    '''\n",
    "    A function to plot images\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    ax.text(0.1,0.85,'Label : {}'.format(digit), fontsize=15, color ='b', transform=ax.transAxes)\n",
    "    _ = ax.imshow(image, cmap='bone_r')\n",
    "    \n",
    "    \n",
    "def mnist_result_anim(images, digits_true, digits_predicted, shape=[28,28], cmap='bone_r', interval=500, repeat=True, n_iterations = None, video_file=None):\n",
    "\n",
    "    try:\n",
    "        _ = digits_true.shape[1]\n",
    "    except:\n",
    "        digits_true = transform_label(digits_true)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,6))\n",
    "    image = ax.imshow(np.zeros([28,28]), cmap=cmap)\n",
    "    true_digit = ax.text(0.1, 0.9, s='True Value:', fontsize=12, transform = ax.transAxes, color='b')\n",
    "    predicted_digit = ax.text(0.1, 0.85, s='PredictedValue:', fontsize=12, transform = ax.transAxes, color='g')\n",
    "\n",
    "    def update(i):\n",
    "        image=ax.imshow(images[i].reshape(shape[0],shape[1]),cmap=cmap)\n",
    "        true_digit.set_text('True Value: {}'.format(np.argmax(digits_true[i])))\n",
    "        predicted_digit.set_text('PredictedValue: {}'.format(np.argmax(digits_predicted[i])))\n",
    "        color = '{}'.format('g' if np.argmax(digits_true[i]) == np.argmax(digits_predicted[i]) else 'r')\n",
    "        predicted_digit.set_color(color)\n",
    "        return image, true_digit, predicted_digit,\n",
    "    if n_iterations is None:\n",
    "        n_iterations = len(images)\n",
    "    anim = animation.FuncAnimation(fig, update, frames=n_iterations, interval=interval, blit=True, repeat=repeat)\n",
    "    if video_file is not None:\n",
    "        anim.save('{video_file}.gif'.format(video_file=video_file),writer='pillow', fps=30)\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:20:38.468260Z",
     "start_time": "2020-09-24T21:20:38.456293Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    '''\n",
    "    A class for configuration of CNN\n",
    "    '''\n",
    "    def __init__(self, batch_size, n_epochs, train_portion, min_delta, stop_patience, lr_patience=None, rnn_nodes = None, repeated_predictions=False, monitor ='loss'):    \n",
    "        self.repeated_predictions = repeated_predictions\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_nodes = rnn_nodes\n",
    "        self.n_epochs = n_epochs\n",
    "        self.train_portion = train_portion\n",
    "        \n",
    "        # Early stop call back for keras\n",
    "        self.early_stop_clbk = tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                                                min_delta=min_delta, \n",
    "                                                                patience=stop_patience,\n",
    "                                                                verbose=0, \n",
    "                                                                mode='auto',\n",
    "                                                                restore_best_weights=True)\n",
    "        if lr_patience is not None:\n",
    "            # Adaptive learning rate call back for keras\n",
    "            self.lr_plan = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                             factor=0.1,patience=lr_patience,\n",
    "                                             verbose=0,\n",
    "                                             mode='auto',\n",
    "                                             min_delta=min_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "9PpH0kFDi2JE",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "LZqDHF2mi2JE",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>In this section, we will use the famous <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a> to build a Convolutional Neural Networks capable to perform handwritten digits classification. CNN will say, with some associated error, what type of digit is the presented input.</p>\n",
    "\n",
    "<h3> Hardware Acceleration</h3>\n",
    "\n",
    "In this notebook we harness the power of hardware acceleration. The main hardware accelerators and the associated software are listed below:\n",
    "\n",
    "    - NVIDIA GPUs\n",
    "        - Software: CUDA, a high level programing language from NVIDIA to help us write programs for graphic\n",
    "        - Card: GTX and Tesla cards. \n",
    "        The memory bandwidth is the main feature of a graphics card that indicate the computation power\n",
    "        to fetch high dimensional data\n",
    "        e.g. GTX 1080 (484 GB/s), P100(730 GB/s)\n",
    "        \n",
    "    - AMD GPUS\n",
    "        -software: OpenCL, not popular for developers working on deep learning libraries right now\n",
    "        \n",
    "    - Google TPUs (Tensor Flow Processing Units)\n",
    "        - Specifically developed for TensorFlow\n",
    "        - TPU promises an acceleration over and above GPU\n",
    "        \n",
    "    - FPGA (Field Programmable Gate Arrays)\n",
    "    \n",
    "  \n",
    "Limitations of GPUs:\n",
    "    - Limited memory capacity. We need to load the data into GPU memory in order to process it. \n",
    "    SO, GPUs are not practical for large data sets\n",
    "    \n",
    "    - Accessibility: Often expensive to have on a local machine\n",
    "    \n",
    "    \n",
    "<h3> TensorFlow GPU Installation</h3>\n",
    "\n",
    "Tensor flow version 1.* had a separate installation for CPU and GPU support. With TensorFlow 2.1, the default tensor flow installation supports both CPU and GPU versions of tensor flow. However this does not address the complexities of steps required to have a working version of TensorFlow with GPU.\n",
    "\n",
    "Here are some steps for installing a working version of TensorFlow 2.1.* on Windows that works when this document was created.\n",
    "\n",
    "Step 1) Make sure you have Microsoft Visual Studio installed (community version should be enough)\n",
    "\n",
    "Step 2) <a href=\"https://developer.nvidia.com/cuda-10.0-download-archive?\">Nvidia CUDA</a>  Toolkit (Version 10.0 was used here)\n",
    "\n",
    "Step 3) Install <a href=\"https://developer.nvidia.com/rdp/cudnn-download\">Nvidia cuDNN</a>  (Use the correct version based on Nvidia CUDA toolkit). Need to create an account and alternatively can use google login. The downloaded artifact is a zip file containing multiple folders. Make sure that the *bin* and *lib\\x64* folders are added to the system/user PATH.\n",
    "\n",
    "Step 4) Create a Python virtual environment and install the correct version of Python that is compatible with the version of TesnorFlow you want to install. Note that each version of TensorFlow 2.* may have different requirement in terms of version of CUDA tool kit.\n",
    "\n",
    "An alternative approach is to use conda becuase Anaconda is supporting TensorFlow 2.0 and you can create an environment with proper installation of tensor flow that matches the hardware, toolkits and DLLs installed on you machine\n",
    "\n",
    "```python\n",
    "conda create -n TfGpu python=3.6 tensorflow-gpu\n",
    "```\n",
    "\n",
    "The last step is to active the environment and start using TensorFlow.\n",
    "\n",
    "```python\n",
    "conda activate TfGpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "3R5RNEiPi2JG",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2 id=\"deep_learning_MNIST\">Deep Learning with CNN applied on MNIST (Using GPU)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "YjEGuGv7i2JH",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>We are going to create a simple CNN to perform classification tasks on the MNIST digits dataset. If you are not familiar with the MNIST dataset, you could read more about it: <a href=\"http://yann.lecun.com/exdb/mnist/\">click here</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "uM9BC2eti2JH",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>What is MNIST?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "MlvBhgX-i2JI",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>According to Lecun's website, the MNIST is a: \"database of handwritten digits that has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from the National Institute of Standards and Technology (NIST). The digits have been size-normalized and centered in a fixed-size image\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "mCrAB31ji2JI",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Import the MNIST dataset using TensorFlow built-in feature</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "y_FAwU3li2JJ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>It's very important to notice that MNIST is a high optimized data-set and it does not contain images. You will need to build your own code if you want to see the real digits. Another important side note is the effort that the authors invested on this data-set with normalization and centering operations.</p>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:20:38.506827Z",
     "start_time": "2020-09-24T21:20:38.477257Z"
    },
    "button": false,
    "colab": {},
    "colab_type": "code",
    "id": "YDjIP0Aui2JJ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluate the GPU device</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:20:39.816889Z",
     "start_time": "2020-09-24T21:20:38.512824Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:20:39.835893Z",
     "start_time": "2020-09-24T21:20:39.820893Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:20:39.851894Z",
     "start_time": "2020-09-24T21:20:39.838896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enabling device placement logging causes any Tensor allocations or operations to be printed.\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:20:41.538839Z",
     "start_time": "2020-09-24T21:20:39.858901Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:22:15.690023Z",
     "start_time": "2020-09-24T21:22:15.138022Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:22:16.035342Z",
     "start_time": "2020-09-24T21:22:15.693033Z"
    }
   },
   "outputs": [],
   "source": [
    "image_plot(x_train[125], y_train[125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:22:18.192671Z",
     "start_time": "2020-09-24T21:22:16.679654Z"
    }
   },
   "outputs": [],
   "source": [
    "# One hot transformation\n",
    "y_train, y_test = transform_label(y_train), transform_label(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "N5NhY00pi2JN",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>The <code>one-hot = True</code> argument only means that, in contrast to Binary representation, the labels will be presented in a way that only one bit will be on for a specific digit.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "pS9e0Ksoi2JN",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Understanding the imported data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_U9wWCji2JO"
   },
   "source": [
    "The imported data can be divided as follow:\n",
    "\n",
    "<ul>\n",
    "    <li>Training (mnist.train):  Use the given dataset with inputs and related outputs for training of NN. In our case, if you give an image that you know that represents a \"nine\", this set will tell the neural network that we expect a \"nine\" as the output.\n",
    "        <ul>\n",
    "            <li>55,000 data points</li>\n",
    "            <li>mnist.train.images for inputs</li>\n",
    "            <li>mnist.train.labels for outputs</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Validation (mnist.validation): The same as training, but now the data is used to generate model properties (classification error, for example) and from this, tune parameters like the optimal number of hidden units or determine a stopping point for the back-propagation algorithm.\n",
    "        <ul>\n",
    "            <li>5,000 data points</li>\n",
    "            <li>mnist.validation.images for inputs</li>\n",
    "            <li>mnist.validation.labels for outputs</li>\n",
    "        </ul>        \n",
    "    </li>\n",
    "    <li>Test (mnist.test): the model does not have access to this informations prior to the test phase. It is used to evaluate the performance and accuracy of the model against \"real life situations\". No further optimization beyond this point.\n",
    "        <ul>\n",
    "            <li>10,000 data points</li>\n",
    "            <li>mnist.test.images for inputs</li>\n",
    "            <li>mnist.test.labels for outputs</li>\n",
    "        </ul>         \n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "_2r2Ljjli2JS",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Input and Output</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGt5leCPi2JS"
   },
   "source": [
    "<b>Input 'X':</b> represents the \"space\" allocated input or the images.\n",
    "<ul>\n",
    "    <li>Each input has 784 pixels distributed by a 28 width x 28 height matrix.</li>\n",
    "    <li>The 'shape' argument defines the tensor size by its dimensions.</li>\n",
    "    <li>1st dimension = None. Indicates that the batch size, can be of any size.</li>\n",
    "    <li>2nd dimension = 784. Indicates the number of pixels on a single flattened MNIST image.</li> \n",
    "</ul>\n",
    "\n",
    "<b>Output 'Y':</b> represents the final output or the labels.  \n",
    "<ul>\n",
    "    <li>10 possible classes (0, 1, 2, 3, 4, 5, 6, 7, 8, 9).</li>\n",
    "    <li>The 'shape' argument defines the tensor size by its dimensions.</li>\n",
    "    <li>1st dimension = None. Indicates that the batch size, can be of any size.</li>\n",
    "    <li>2nd dimension = 10. Indicates the number of targets/outcomes.</li> \n",
    "</ul>\n",
    "\n",
    "<p><b>dtype for both placeholders:</b> if you not sure, use tf.float32. The limitation here is that the later presented softmax function only accepts float32 or float64 dtypes. For more dtypes, check TensorFlow's documentation <a href=\"https://www.tensorflow.org/api_docs/python/tf/dtypes/DType\">here</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "JFUd5JSDi2JV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Convolutional neural networks (CNNs)</h3>\n",
    "\n",
    "<p>Convolutional neural networks (CNNs) is a type of feed-forward neural network, consist of multiple layers of  neurons that have trainable weights and biases. Each neuron in a layer that receives some input, process it, and optionally follows it with a non-linearity. The network has multiple layers such as convolution, max pool, drop out and fully connected layers. In each layer, small neurons process portions of the input image. The outputs of these collections are then tiled so that their input regions overlap, to obtain a higher-resolution representation of the original image; and it is repeated for every such layer. The important point here is: CNNs are able to break the complex patterns down into a series of simpler patterns, through multiple layers.</p>\n",
    "\n",
    "The covolutional layer is in fact a moving averaging window with a a trainable kernel/weights that can be used to capture certain features of an image or video or an array. The convolution of f and g is written f∗g. It is defined as the integral/summation of the product of the two functions after one is reversed and shifted.\n",
    "\n",
    "$$\n",
    "f * g (t) = \\int f(\\tau)g(t-\\tau) \\, d\\tau\n",
    "$$\n",
    "\n",
    "An example for architecture of a typical deep CNN network is:\n",
    "<ul> \n",
    "    <li>Input [?, 28, 28, 1]: 2D image with one input channel (gray scale vs three channel for RGB). This is known as NHWC format (number of images, height, width and channels)</li>\n",
    "    <li> First convolutional layer: Apply 32 filter of [5x5]. The kernel averaging window size is 5x5 and we selected to have 32 output channels. The number of trainable parameters (i.e. wights and biases) is (5\\*5\\*1 + 1)\\*32</li>\n",
    "    <li>Output of the first convolutional layer: [?, 28, 28, 32] depending on definition of strides and padding, the output size may or may not match the input size.</li>\n",
    "    <li>Applying the activation function (ReLU 1): The output shape is not changed i.e. [?, 28, 28, 32]</li>\n",
    "    <li>Pooling layer (i.e. Max pooling 1): Pool size determines the size of pooling window and strides define the output size. With the pool size of 2 and strides of 2, the output shape will be [?, 14, 14, 32]</li>\n",
    "    <li>Convolutional layer 2: 64 kernels with the size of 5x5 results in output shape of [?, 14, 14, 64]. Number of parameters is (5\\*5\\*32 + 1)\\*64 i.e. kernel size \\* number of input channels \\* number of output channels +number of biases (1 per output channel) </li>\n",
    "    <li>Non linear activation (i.e. ReLU 2): The output shape is not changed i.e. [?, 14, 14, 64]</li>\n",
    "    <li> Pooling (i.e. Max pooling 2): With strides of size 2 along width and height  [?, 7, 7, 64]</li>\n",
    "    <li>Flatten layer: Convert the 2D image and the output channels from the previous layer into a flat array. The output size is [? 3136(7\\*7\\*64)]</li>\n",
    "    <li>fully connected layer (i.e layer 3): Fully connected layer with 1024 neurons results in an output with shape of [? 1024]. The number of parameters is 3136*1024 + 1024</li>\n",
    "    <li>Activation layer (i.e. ReLU 3):  Output shape remains [? 1024]</li>\n",
    "    <li>Drop out layer to implement generalization: Output shape remains [? 1024]</li>\n",
    "    <li>fully connected layer (i.e. layer 4): Output shape of [? 10] that maps to the 10 classes of digits from 0 to 9</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T14:11:56.187602Z",
     "start_time": "2020-09-21T14:11:56.161598Z"
    }
   },
   "source": [
    "<img src=\"https://ibm.box.com/shared/static/vn26neef1nnv2oxn5cb3uueowcawhkgb.png\" style=\"width:800px; height:400px;\" alt=\"HTML5 Icon\" >\n",
    "\n",
    "<h4>Convolve with weight tensor and add biases.</h4>\n",
    "\n",
    "Inputs:\n",
    "<ul>\n",
    "    <li>Tensor of shape [batch, in_height, in_width, in_channels]. input (i.e. x) of shape [batch_size,28 ,28, 1].</li>\n",
    "    <li>A filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]. W is of size [5, 5, 1, 32].</li>\n",
    "    <li>Stride which is  [1, 1, 1, 1]. The convolutional layer, slides the \"kernel window\" across the input tensor. As the input tensor has 4 dimensions:  [batch, height, width, channels], then the convolution operates on a 2D window on the height and width dimensions. <b>strides</b> determines how much the window shifts by in each of the dimensions. As the first and last dimensions are related to batch and channels, we set the stride to 1. But for second and third dimension, we could set other values, e.g. [1, 2, 2, 1].</li>\n",
    "</ul>\n",
    "    \n",
    "Process:\n",
    "<ul>\n",
    "    <li>Change the filter to a 2-D matrix with shape [5\\*5\\*1, 32].</li>\n",
    "    <li>Extracts image patches from the input tensor to form a <i>virtual</i> tensor of shape <code>[batch, 28, 28, 5\\*5\\*1]</code>.</li>\n",
    "    <li>For each batch, right-multiplies the filter matrix and the image vector.</li>\n",
    "</ul>\n",
    "\n",
    "Output:\n",
    "<ul>\n",
    "    <li>A <code>Tensor</code> (a 2-D convolution) of size (?, 28, 28, 32).</li>\n",
    "    <li>Notice: the output of the first convolution layer is 32 [28x28] images. Here 32 is considered as volume/depth of the output image.</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/iizf4ui4b2hh9wn86pplqxu27ykpqci9.png\" style=\"width:800px;height:400px;\" alt=\"HTML5 Icon\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Stride! How does it work?</h4>\n",
    "\n",
    "Stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video an can be used for compression. For example, if a neural network's stride is set to 1, the filter will move one pixel, or unit,  at a time. The size of the filter affects the encoded output volume, so stride is often set to a whole integer, rather than a fraction or decimal.\n",
    "\n",
    "<img src=\"https://images.deepai.org/django-summernote/2019-06-03/56e53bc1-bac3-48f4-a08c-dce77a57464b.png\">\n",
    "\n",
    "\n",
    "Imagine a convolutional neural network is taking an image and analyzing the content. If the filter size is 3x3 pixels, the contained nine pixels will be converted down to 1 pixel in the output layer. Naturally, as the stride, or movement, is increased, the resulting output will be smaller. Stride is a parameter that works in conjunction with padding, the feature that adds blank, or empty pixels to the frame of the image to allow for a minimized reduction of size in the output layer. Roughly, it is a way of increasing the size of an image, to counteract the fact that stride reduces the size. Padding and stride are the foundational parameters of any convolutional neural network.\n",
    "\n",
    "<img src=\"https://images.deepai.org/django-summernote/2019-06-03/5f9e90ca-1405-4fcb-9d43-071d7f710950.png\">\n",
    "\n",
    "\n",
    "As was explained above, <b>stride</b> determines how much the window shifts by in each of the dimensions. For image classification, stride is often a tuple with 4 dimensions. As the first and last dimensions are related to batch and channels, we set the stride to 1. But for second and third dimension, we could set other values, e.g. [1, 2, 2, 1] This means that the convolutional filter/kernel moves 2 cell along width and height of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the ReLU activation Function</h4>\n",
    "\n",
    "<p>In this step, we just go through all outputs convolution layer, <b>convolve1</b>, and wherever a negative number occurs,we swap it out for a 0. It is called ReLU activation Function.</p> \n",
    "<p>Let f(x) is a ReLU activation function $f(x) = max(0,x)$.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Apply the max pooling</h4>\n",
    "\n",
    "<p><b>max pooling</b> is a form of non-linear down-sampling. It partitions the input image into a set of rectangles and, and then find the maximum value for that region.</p>\n",
    "\n",
    "<p>Lets use <b>tf.nn.max_pool</b> function to perform max pooling. \n",
    "<b>Kernel size:</b> 2x2 (if the window is a 2x2 matrix, it would result in one output pixel).</p>\n",
    "    \n",
    "<p><b>Strides:</b> dictates the sliding behaviour of the kernel. In this case it will move 2 pixels everytime, thus not overlapping. The input is a matrix of size 28x28x32, and the output would be a matrix of size 14x14x32.</p>\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/kmaja90mn3aud9mro9cn8pbbg1h5pejy.png\" alt=\"HTML5 Icon\" style=\"width:800px; height:400px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Fully Connected Layer</h4>\n",
    "\n",
    "<p>The fully connected layer is needed to use the Softmax and create the probabilities in the end. Fully connected layers take the high-level filtered images from previous layer, that is all 64 matrices, and convert them to a flat array.</p>\n",
    "\n",
    "<p>So, each matrix [7x7] will be converted to a matrix of [49x1], and then all of the 64 matrix will be connected, which make an array of size [3136x1]. This will connect into another layer of size [1024x1]. So, the weight between these 2 layers will be [3136x1024].</p>\n",
    "\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/pr9mnirmlrzm2bitf1d4jj389hyvv7ey.png\" alt=\"HTML5 Icon\" style=\"width:800px; height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:41:12.825622Z",
     "start_time": "2020-09-22T14:41:12.820670Z"
    }
   },
   "source": [
    "<h4>SoftMax Activation Function</h4>\n",
    "\n",
    "The softmax function, also known as softargmax or normalized exponential function is a generalization of the logistic function to multiple dimensions. It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.\n",
    "\n",
    "Consider an input vector of **Z** of K real numbers. Applying the soft max function results in a each component to be in the interval (0, 1) and sum to 1.  \n",
    "\n",
    "$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}}$\n",
    "\n",
    "For the MNIST data set, the output layer of the network is an array of length 10 to indicate the probability of each digit and soft max activation is used to normalize the output and transform it to a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Categorical Cross Entropy Loss</h4>\n",
    "\n",
    "The categorical cross entropy loss function quantifies the miss classification between two arrays that represent the probability distribution of different classes.\n",
    "\n",
    "Loss = $ -\\sum_{i=1}^{K} y_i.log\\hat{y}_j$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/0*YRAt7P06fL7TObX-.png\" style=\"width:600px; height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>High level over view of CNN</h4>\n",
    "\n",
    "Image below shows a summary of CNN design\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/0*GDlFcyG1CV2OGEbX.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-21T13:58:18.192444Z",
     "start_time": "2020-09-21T13:58:18.181440Z"
    }
   },
   "source": [
    "<h3>Implementing the network architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:22:21.515402Z",
     "start_time": "2020-09-24T21:22:21.417116Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, Dropout, GRU, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.callbacks import LambdaCallback, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "\n",
    "# No need to add the overhead of getting device log anymore\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Parameters and Settings</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:22:22.257160Z",
     "start_time": "2020-09-24T21:22:22.253165Z"
    }
   },
   "outputs": [],
   "source": [
    "width = 28 # width of the image in pixels \n",
    "height = 28 # height of the image in pixels\n",
    "flat = width * height # number of pixels in one image \n",
    "class_output = 10 # number of possible classifications for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:22:22.694026Z",
     "start_time": "2020-09-24T21:22:22.689086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape the image to ? x 28 x 28 x 1\n",
    "if x_train.shape[-1] !=1:\n",
    "    x_train = x_train[:, :, :, np.newaxis]\n",
    "    x_test = x_test[:, :, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Using keras to design the network</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:22:25.387348Z",
     "start_time": "2020-09-24T21:22:24.691912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creat the model\n",
    "model = Sequential(name='CnnSequential')\n",
    "\n",
    "# Adding the first CNN layer\n",
    "#-------------------------------------------\n",
    "model.add(Conv2D(filters= 32\n",
    "                 ,kernel_size = (5, 5)\n",
    "                 ,strides = (1,1)\n",
    "                 ,input_shape=(28, 28, 1) # Width, Height and number of input channels. The number of batches is implicit\n",
    "                 ,activation='relu' # Rectifie linear (the nonlinear activation function)\n",
    "                 ,padding='same' # Padding for the edges of the image as the kernel moves\n",
    "                 ,name='ConvLayer1')) \n",
    "# Adding the max pool process to compress image\n",
    "model.add(MaxPool2D(pool_size = [2,2], strides=(2,2), padding='same', name='MaxPool1'))\n",
    "\n",
    "# Adding the second CNN layer\n",
    "#-------------------------------------------\n",
    "model.add(Conv2D(filters= 64\n",
    "                 ,kernel_size = (5, 5)\n",
    "                 ,strides = (1,1)\n",
    "                 ,activation='relu' # Rectifie linear (the nonlinear activation function)\n",
    "                 ,padding='same' # Padding for the edges of the image as the kernel moves\n",
    "                 ,name='ConvLayer2')) \n",
    "\n",
    "# Adding the max pool process to compress image\n",
    "model.add(MaxPool2D(pool_size = [2,2], strides=(2,2), padding='same', name='MaxPool2'))\n",
    "\n",
    "# Flatten the process images after CNNs with all the channels\n",
    "#-----------------------------------------------------------------\n",
    "model.add(Flatten(name='Flatten'))\n",
    "\n",
    "# Adding a dense layer before the final output\n",
    "#-----------------------------------------------------------------\n",
    "model.add(Dense(1024, activation='relu', name='Dense'))\n",
    "\n",
    "# Adding a drop out process\n",
    "#-----------------------------------------------------------------\n",
    "model.add(Dropout(rate=0.1, name='DropOut'))\n",
    "\n",
    "# The final layer to map to 10 classes\n",
    "#-----------------------------------------------------------------\n",
    "model.add(Dense(10, activation='softmax', name='Output'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Nadam(learning_rate=0.001), \n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Review the number of parameters for each layer</h4>\n",
    "\n",
    "- First CNN: size of the kernel \\* number of input channels \\* number of out put channels + Number of biases (1 per output channel) e.g. 5\\*5\\*1\\*32 + 32 = 832\n",
    "\n",
    "- Second CNN: size of the kernel \\* number of input channels \\* number of out put channels + Number of biases (1 per output channel) e.g. 5\\*5\\*32\\*64 + 64 = 51264\n",
    "\n",
    "- Flatten layer: flatten image size \\* number of channels e.g. 7\\*7\\*64 = 3136\n",
    "\n",
    "- Dense layer: Number of neurons from previous layer \\* number of neurons on current layer + biases (one per output neuron) e.g. 3136\\*1024 + 1024 = 3212288\n",
    "\n",
    "- Last layer: Number of neurons from previous layer \\* number of neurons on current layer + biases (one per output neuron) e.g. 1024\\*10 + 10 = 10250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:23:19.134645Z",
     "start_time": "2020-09-24T21:23:19.128640Z"
    }
   },
   "outputs": [],
   "source": [
    "# Central configuration\n",
    "config = Config(batch_size=500, n_epochs = 30, train_portion = 0.7, min_delta=0.001, stop_patience=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:30:16.243256Z",
     "start_time": "2020-09-24T21:23:20.336641Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=config.n_epochs, \n",
    "                    batch_size=config.batch_size,\n",
    "                    validation_data=(x_test, y_test), \n",
    "                    callbacks=[config.early_stop_clbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Visualizations</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:34:12.913481Z",
     "start_time": "2020-09-24T21:34:04.546949Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:34:45.914701Z",
     "start_time": "2020-09-24T21:34:12.927492Z"
    }
   },
   "outputs": [],
   "source": [
    "# animation of image vs prediction for test data set\n",
    "mnist_result_anim(x_test[0:20],y_test[0:20], prediction[0:20], interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:34:50.968931Z",
     "start_time": "2020-09-24T21:34:45.921701Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(map(np.argmax, y_test)), list(map(np.argmax, prediction)))\n",
    "df_cm = pd.DataFrame(cm, range(10), range(10))\n",
    "\n",
    "# trace of the confusion matrix is the count of correct calssifications\n",
    "accuracy = (np.trace(cm) / np.sum(cm)) * 100\n",
    "\n",
    "\n",
    "sn.set(font_scale=1.4)\n",
    "fig, ax = plt.subplots(figsize=(15,12))\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, ax=ax, cmap='viridis')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.text(0.01, 1.01, s='Test Accuracy: %{:.2f}'.format(accuracy), fontsize=14, transform = ax.transAxes, color='g')\n",
    "ax.set_title('Confusion Matrix for Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Get the layer output</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:35:01.064925Z",
     "start_time": "2020-09-24T21:35:01.057925Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:35:01.849921Z",
     "start_time": "2020-09-24T21:35:01.843920Z"
    }
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:35:57.257620Z",
     "start_time": "2020-09-24T21:35:03.086920Z"
    }
   },
   "outputs": [],
   "source": [
    "# input placeholder\n",
    "network_input = model.input\n",
    "\n",
    "# all layer outputs\n",
    "outputs = [layer.output for layer in model.layers] \n",
    "\n",
    "# evaluation functions\n",
    "functors = [backend.function([network_input], [out]) for out in outputs]\n",
    "# Note: If drop out exists for the input layer, functors = [backend.function([inp, backend.learning_phase()], [out]) for out in outputs]\n",
    "\n",
    "# Evaluate outputs given the test data\n",
    "layer_outs = [func([x_test]) for func in functors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer_outs is a list and it has 8 items for the 8 layers of the CNN designed above:\n",
    "\n",
    "- ConvLayer1\n",
    "- MaxPool1\n",
    "- ConvLayer2\n",
    "- MaxPool2\n",
    "- Flatten\n",
    "- Dense\n",
    "- DropOut\n",
    "- Output\n",
    "\n",
    "Each item from layer_outs, is a list with one item and that item is an array. For, a convolutional layer before and after pooling the shpae of that array is [n_batch, image_width, image_height, n_output_Channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-24T21:38:12.453129Z",
     "start_time": "2020-09-24T21:38:09.145128Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose a random test input image from test data set\n",
    "image_number = np.random.randint(0, x_test.shape[0])\n",
    "\n",
    "# a random output channel for each convolutional layer based on number of filters for that layer\n",
    "filter_1 = np.random.randint(0,32)\n",
    "filter_2 = np.random.randint(0,64)\n",
    "\n",
    "fig, ax = plt.subplots(1,5,figsize=(25,4))\n",
    "\n",
    "image1 = ax[0].imshow(x_test[image_number][:,:,0], cmap='bone_r')\n",
    "ax[0].set_title('Input Image')\n",
    "\n",
    "image2 = ax[1].imshow(layer_outs[0][0][image_number][:,:,filter_1], cmap='bone_r')\n",
    "ax[1].set_title('Convolution Layer 1')\n",
    "\n",
    "image3 = ax[2].imshow(layer_outs[1][0][image_number][:,:,filter_1], cmap='bone_r')\n",
    "ax[2].set_title('Convolution Layer 1 (After Max Pool)')\n",
    "\n",
    "image4 = ax[3].imshow(layer_outs[2][0][image_number][:,:,filter_2], cmap='bone_r')\n",
    "ax[3].set_title('Convolution Layer 2')\n",
    "\n",
    "image5 = ax[4].imshow(layer_outs[3][0][image_number][:,:,filter_2], cmap='bone_r')\n",
    "ax[4].set_title('Convolution Layer 2 (After Max Pool)')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "include_colab_link": true,
   "name": "CNN-MNIST-Dataset_CPU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
